- do_first_eval=False
- mask_path=./none/0.0_mask.pt
- loss=sft
- model=llama7b
- model.name_or_path=/root/Tests/DLTH_LoTA/models/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b
- model.archive=/root/Tests/DLTH_LoTA/models/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b
- datasets=[gsm8k]
- exp_name=gsm8k_/root/Tests/DLTH_LoTA/models/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b_10_5e-7_8_1.0_none_0.0
- eval_batch_size=16
- sample_during_eval=false
- lr=5e-7
- trainer=BasicTrainer
- activation_checkpointing=True
- data_fraction=1.0
- save_every=epoch_3
- eval_every=100000
- n_epochs=3
- batch_size=8
- gradient_accumulation_steps=1
- optimizer=RMSprop
- grad_norm_strategy=even
- max_grad_norm=10
- flip_mask=false
- freeze_odd_layers=false
- freeze_even_layers=false
