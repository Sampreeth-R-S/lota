- do_first_eval=False
- mask_path=./none/0.0_mask.pt
- loss=sft
- model=llama7b
- model.archive=mistralai/Mistral-7B-v0.1
- datasets=[gsm8k]
- exp_name=gsm8k_mistralai/Mistral-7B-v0.1_10_5e-7_8_1.0_none_0.0
- eval_batch_size=16
- sample_during_eval=false
- lr=5e-7
- trainer=BasicTrainer
- activation_checkpointing=True
- data_fraction=1.0
- save_every=epoch_3
- eval_every=100000
- n_epochs=3
- batch_size=8
- gradient_accumulation_steps=1
- optimizer=RMSprop
- grad_norm_strategy=even
- max_grad_norm=10
- flip_mask=false
- freeze_odd_layers=false
- freeze_even_layers=false
