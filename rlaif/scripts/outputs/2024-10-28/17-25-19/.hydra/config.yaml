fast_forward: 0
scores_path: null
ckpt_path: null
grad_norm_strategy: even
lora_rank: 8
lora_alpha: 32
freeze_odd_layers: false
freeze_even_layers: false
flip_mask: false
seed: 0
exp_name: gsm8k_meta-llama/Llama-2-7B_10_5e-7_8_1.0_none_0.0_LORA_8_32
batch_size: 8
eval_batch_size: 16
debug: false
fsdp_port: 12355
datasets:
- gsm8k
wandb:
  enabled: true
  entity: null
  project: dpo-rlaif
local_dirs:
- /home/ubuntu/.cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: false
local_run_dir: ${get_local_run_dir:${exp_name},${local_dirs}}
lr: 5.0e-07
gradient_accumulation_steps: 1
max_grad_norm: 10
max_length: 512
max_prompt_length: 256
n_epochs: 3
n_examples: null
n_eval_examples: 64
trainer: BasicTrainer
optimizer: RMSprop
warmup_steps: 0
activation_checkpointing: true
eval_every: 100000
minimum_log_interval_secs: 1.0
save_every: epoch_3
trigger_alpaca_eval: false
eval_gpu: 6
prefs_path: null
mask_path: ./masks/none/0.0_mask.pt
save_snip_mask_path: null
snip_sparsity: 0.9
snip: false
num_turns: 1
data_fraction: 1.0
model:
  name_or_path: meta-llama/Llama-2-7b-hf
  tokenizer_name_or_path: null
  archive: meta-llama/Llama-2-7B
  block_name: LlamaDecoderLayer
  policy_dtype: float32
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: sft
